---
title: "A quasi-mathematical way to make sense of metrics in technical organizations"
date: 2026-02-07
permalink: /posts/attention/
tags:
  - metrics
  - leadership
---

_THIS POST IS CURRENTLY UNDER CONSTRUCTION_

Introduction
------------

The practice of using data to rationally guide decisions and to draw conclusions has arguably been the driving force that has brought human civilization from a subsistance existance to one characterized by comfort and abundance (relatively speaking!). For good reason, the importance of data-driven reasoning has been deeply instilled into the culture of quantiative and technical disciplines. 

Concurrently, the systems that we both build and operate within are too complex to fit, in their entirety, within our meager human minds. We need some way to measure them in order to understand them. The mathematician and physicist, [Lord Kelvin](https://en.wikipedia.org/wiki/Lord_Kelvin), articulated this clearly [in 1883](https://en.wikiquote.org/wiki/William_Thomson):

<div style="font-style: italic; max-width: 500px; padding-left: 15px; border-left: 3px solid #ccc;">I often say that when you can measure what you are speaking about, and express it in numbers, you know something about it; but when you cannot measure it, when you cannot express it in numbers, your knowledge is of a meagre and unsatisfactory kind; it may be the beginning of knowledge, but you have scarcely, in your thoughts, advanced to the stage of science, whatever the matter may be. ~ Lord Kelvin</div>

<br>

The combination of both our secular cultural traditions, and the simple need to understand complex systems, lead technical organizations to fixate on **metrics** (a metric defined here to simply be a measured quantity tracked over time). This reliance on metrics is deeply ingrained. Managers repeat quotes like, "You can't manage what you can't measure." Software engineers create dashboards and databases to track metrics. Meetings often begin with an overview of where the project or business stands in terms of the metrics.

I would by no means be the first to point out that an over-reliance on metrics can lead to poor outcomes. For example, [Goodhart's Law](https://en.wikipedia.org/wiki/Goodhart%27s_law) is an adage that goes, "When a measure becomes a target, it ceases to be a good measure." [Jeff Bezos](https://en.wikipedia.org/wiki/Jeff_Bezos) did well in articulating the process by which a metric "ceases to be a good measure." In a [recent podcast interview](https://www.youtube.com/watch?v=8ij964FCQiw), Bezos describes how organizations can target a metric as a _proxy_ for seomthing the organization _actually_ value. Over time, that proxy becomes a poor approximation for that true thing of value:

<div style="font-style: italic; max-width: 500px; padding-left: 15px; border-left: 3px solid #ccc;">One of the things that happens in business is that you develop certain things that you’re managing to. The typical case would be a metric. And that metric isn’t the typical underlying thing. Maybe the metric is something like the number of returns you get per unit sold… And so what happens is a little bit of a kind of inertia sets in where somebody a long time ago invented that metric… and they had a reason why they chose that metric. And then fast forward five years and that metric becomes the proxy for truth… Let’s say that metric is a proxy for customer happiness. That metric is not actually customer happiness; it’s a proxy for customer happiness. The person who invented the metric understood that connection. Five years later… you forget the truth behind why you were watching that metric in the first place. And then the world shifts a little bit. And now that proxy isn’t as valuable as it used to be. Or it’s missing something. ~ Jeff Bezos</div>

<br>

In this blog post, I would like to delve a bit deeper into the ways in which metrics can be used and misused by technical organizations. To do so, I will develop a sort of quasi-mathematical framework for thinking about metrics in way that has brought me clarity. 

To spoil the punchline, I will frame a system as a complex [high-dimensional](https://mbernste.github.io/posts/intrinsic_dimensionality/) object -- the system under consideration might be a business, service, process, or technology. There exists some theoretical "value function" that maps systems to a number that tells us how "good" that system is. We don't necessarily know what that function looks like, so we develop metrics, which are themselves projection of the high-dimensional system onto the real number line. Metrics can then be categorized by their properties, and by their relationship to the true value function, from which we can reason about what makes some metrics more prone to misues than others. Let's get started.

Systems as high-dimensional objects
-----------------------------------

Before we get to discussing metrics, we will first generalize the system being measured as a [high-dimensional](https://mbernste.github.io/posts/intrinsic_dimensionality/) object in some abstract space (akin to a [vector space](https://mbernste.github.io/posts/vector_spaces/)). By "system", I mean any complicated thing that a given technical organization seeks to understand or improve. 

For example, such a system under consideration might be an entire business; Businesses are complicated "high-dimensional" objects in that they have many components: employees, processes, capital, debt, revenue, and so on. A piece of technology, like a website, that an organization is building is also such a system. For example, a website has many components: traffic, latency, lines of code, vulnerability, etc. A machine learning algorithm is also such a system with many characteristics: accuracy, precision, recall, computational cost, etc.

In a very abstract way, one can imagine that any given system resides in a "space" comprising other similar systems. For simplicity, let's take a business: We can summarize a business in terms of a large list of numbers like number of employees, sales per month, cost of goods sold, cash on hand, debt... (the list can go on an on). Given such a list (which could be extremely long), we can place the business in a coordinate vector space where each location in the vector space is some (possibly non-existent) business. A schematic showing three dimensions is shown below:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/business_as_vectors_metrics.png" alt="drawing" width="550"/></center>

We will denote this space of possible systems (e.g., businesses) as $\mathcal{X}$. A given system $x$ is a member of $\mathcal{X}$, denoted $x \in \mathcal{X}$.


Value functions tell you how "good" a system is
-----------------------------------------------

In many cases, the goal of an organization to either improve the system under measurement or, at the very least, to assess how "good" the system is in terms of some subjective or economic measure of value. 

At risk of being pedantic, let's define a **value function** to be a function $V$ that maps systems in $\mathcal{X}$ to real numbers that describe the value of that system:

$$V : \mathcal{X} \rightarrow \mathbb{R}$$

That is, $V(x)$ tells us how much to value $x$. If we have two systems, $x_1$ and $x_2$, then $V(x_1) > V(x_2)$ tells us we should prefer $x_1$ to $x_2$. We can depict this schematically in a small, two-dimensional space of systems with a heatmap. Below is a toy example:


In some situations, the value function might be quite clear. For example, when considering a business, the value function might simply be its _profitability_ -- that is, its revenue minus cost. However, in other cases, it is not so clear cut. A premier example in this situation may be judging aesthetic value. For example, say we are a film studio where we seek to assess which movie script we should fund. It goes without saying that, if we consider $\mathcal{X}$ to be the space of movie scripts, the hypothetical function, $V$, would be quite difficult to define (that said, [film studios are trying](https://www.theverge.com/2019/5/28/18637135/hollywood-ai-film-decision-script-analysis-data-machine-learning)!).

The example of a film studio is quite extreme; I think most people would take as obvious that judging objects of artistic expression cannot be approached quantitively. However, I would argue that even in technical fields, where the system under study _should_ admit an easy-to-define value function, it often is not as clear-cut as one would hope. Take for example an algorithm that automates making medical diagnoses -- say, an algorithm for detecting tumors in medical imaging data.

Metrics are functions that "project" systems into lower dimensions
------------------------------------------------------------------

Metrics can be placed into two distinct categories
--------------------------------------------------

### Exploratory metrics

### Value-measuring metrics


Not all systems admit an accurate value-measuring metric
--------------------------------------------------------

Sometimes, because the value function is so complex, it is just not possible to develop a metric that tracks it accurately enough to be relied upon. In such cases, it is often wise to admit this outright rather than expending valuable time and energy on finding such an elusive value-measuring metric. The worst thing one can do is to choose a metric that is a poor approximator of real value. Relying on a poor value-approximator is a road to ruin and is exactly what Bezos warns against in his message on "proxies". 

Sometimes, one just has to admit defeat: We can't quantify "good" even though we know it when we see it.

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/metric_meme.png" alt="drawing" width="500"/></center>

<br>

This is indeed a challenging situation to find oneself. It means that one cannot rely upon an easy, automated way to assess the state of the system and to make decisions. One may feel lost at sea without a compass! But, I would argue Dear Reader, that it can be navigated!  




