---
title: 'Variational autoencoders'
date: 2022-11-25
permalink: /posts/vae/
tags:
  - tutorial
  - deep learning
  - machine learning
  - probabilistic models
---

_THIS POST IS CURRENTLY UNDER CONSTRUCTION_

Introduction
------------

Variational autoencoders (VAEs), introduced by [Kingma and Welling (2013)](https://arxiv.org/abs/1312.6114_) are a class of probabilistic models that find latent, low-dimensional representations of data. VAEs are thus a method for performing [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) to reduce data down to their [intrinsice dimensionality](https://mbernste.github.io/posts/intrinsic_dimensionality/). 

There are two complimentary ways of viewing variational autoencoders:
1. **Probabilistic generative model:** VAEs can be viewed as a probabilistic generative model of independent, identically distributed samples, $\boldsymbol{x}_1, \dots, \boldsymbol{x}_m$. Each sample, $\boldsymbol{x}_i$, is associated with a latent (i.e. unobserved), lower-dimensional representation $\boldsymbol{z}_i$. Notably, each observed sample $\boldsymbol{x}_i$ is of higher dimension than its associated lower dimensional representation $\boldsymbol{z}_i$.  Variational autoencoders define a joint distribution $p(\boldsymbol{x}, \boldsymbol{z})$ and enables efficient computation of:
  * An approximation to the posterior $p(\boldsymbol{z} \mid \boldsymbol{x})$
  * The conditional distribution $p(\boldsymbol{x} \mid \boldsymbol{z})$
2. **Autoencoder:** As their name suggests, a VAE can be viewed as an autoencoder. Unlike standard autoencoders, which are deterministic, VAEs are probabilistic; Given an input sample, $\boldsymbol{x}_i$, its encoded, latent representation $\boldsymbol{z}_i$ is randomly generated.

In this blog post we will present VAEs through both of these lenses. We will present an example of running VAEs on MNIST. Finally, we will discuss applications of VAEs in computational biology. Specfically, we will discuss the [scVI method](https://docs.scvi-tools.org/en/stable/user_guide/models/scvi.html) by [Lopez et al. (2018)](https://www.nature.com/articles/s41592-018-0229-2), which is a VAE-based tool used to analyze [single-cell RNA-seq](https://en.wikipedia.org/wiki/Single_cell_sequencing) data.

VAEs as probabilistic generative models
---------------------------------------

At their core, VAEs describe families of probability distributions over real-valued vectors in $\mathbb{R}^J$.  The generative process behind the VAE is as follows: to generate a given sample $\boldsymbol{x} \in \mathbb{R}^J$, we first generate a latent sample  $\boldsymbol{z} \in \mathbb{R}^{D}$ of lower dimension (i.e.,  $D < J$) which will be used to construct $\boldsymbol{x}$. Typically, $\boldsymbol{z}$ is made to follow a standard normal distribution:

$$\boldsymbol{z} \sim N(\boldsymbol{0}, \boldsymbol{I})$$

Then, we use $\boldsymbol{z}$ to construct the parameters, $\boldsymbol{\psi}$, of another distribution used to sample $\boldsymbol{x}$. Crucially, we construct $\psi$ from $\boldsymbol{z}$ using neural networks:

$$\begin{align*} \boldsymbol{\psi} &:= f_{\theta}(\boldsymbol{z}) \\ \boldsymbol{x} &\sim \mathcal{D}(\boldsymbol{\psi}) \end{align*}$$

where $\mathcal{D}$ is a parametric distribution and $f$ is a neural network parameterized by a set of parameters $\theta$. As one example, $\mathcal{D}$ may be a Gaussian distribution with unit variance and $\psi$ is the mean of this distribution.  Here's a schematic illustration of the generative process:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_generative_process_shapes.png" alt="drawing" width="700"/></center>

&nbsp;

This generative process can be visualized graphically below:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_generative_process.png" alt="drawing" width="700"/></center>

&nbsp;

Interestingly, this model enables use to fit very complicated distributions! That's because although the distribution over $\boldsymbol{z}$ and the conditional distribution of $\boldsymbol{x}$ given $\boldsymbol{z}$ may be simple (e.g., both simply normal distributions), the marginal distribution of $\boldsymbol{x}$ becomes complex:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_marginal.png" alt="drawing" width="350"/></center>

&nbsp;

This complexity is a result of the non-linear mapping between $\boldsymbol{z}$ and $\psi$ implemented via the neural network!

Inference
---------

Now, let's say we are given a dataset consisting of data points $\boldsymbol{x}_1, \dots, \boldsymbol{x}_m \in \mathbb{R}^J$ that we assume was generated by a VAE. We may be interested in two central tasks:
1. For fixed $\theta$, for each $\boldsymbol{x}\_i$, compute the posterior distribution $p_{\theta}(\boldsymbol{z}_i \mid \boldsymbol{x}_i)$
2. Find the maximum likelihood estimates of $\theta$

Unfortunately, for a fixed $\theta$, solving for the posterior $p_{\theta}(\boldsymbol{z}_i \mid \boldsymbol{x}_i)$ using Bayes Theorem is intractible due the denominator in the formula for Bayes Theorem requires marginalizing over $\boldsymbol{z}_i$:

$$p_\theta(\boldsymbol{z}_i \mid \boldsymbol{x}_i) = \frac{p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}_i)p(\boldsymbol{z}_i)}{\int p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) \ d\boldsymbol{z}_i }$$

Similarly, it is difficult to estimate the values for $\theta$ by maximizing the data likelihood due to the need to marginalize over each $\boldsymbol{z}\_i$:

$$\begin{align*}\hat{\theta} &:= \text{argmax}_\theta \prod_{i=1}^m p_\theta(\boldsymbol{x}_i) \\ &= \text{argmax}_\theta \prod_{i=1}^m \int p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) \ d\boldsymbol{z}_i  \end{align*}$$

Variatonal autoencoders find approximate solutions to these intractible inference problems using [variational inference](https://mbernste.github.io/posts/variational_inference/). 

First, let's assume that $\theta$ is fixed and attempt to approximate $p\_\theta(\boldsymbol{z}\_i \mid \boldsymbol{x}\_i)$. Variational inference is a method for performing such approximations by first choosing a set of probability distributions, $\mathcal{Q}$, called the _variational family_, and then finding the distribution $q(\boldsymbol{z}\_i) \in \mathcal{Q}$ that is "closest to" $p\_\theta(\boldsymbol{z}\_i \mid \boldsymbol{x}\_i)$. Variational inference uses the [KL-divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between $q(\boldsymbol{z}\_i)$ and $p_\theta(\boldsymbol{z}\_i \mid \boldsymbol{x}\_i)$ as its measure of "closeness". Thus, the goal of variational inference is to minimize the KL-divergence. It turns out that the task of minimizing the KL-divergence is equivalent to the task of maximizing a quantity called the [evidence lower bound (ELBO)](https://mbernste.github.io/posts/elbo/), which is defined as

$$\begin{align*} \text{ELBO}(q) &:= E_{\boldsymbol{z}_1, \dots, \boldsymbol{z}_m  \overset{\text{i.i.d.}}{\sim} q}\left[ \sum_{i=1}^m \log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \sum_{i=1}^m \log q(\boldsymbol{z}_i) \right] \\ &= \sum_{i=1}^m E_{z_i \sim q} \left[\log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \log q(\boldsymbol{z}_i) \right] \end{align*}$$

Thus, variational inference entails finding

$$\hat{q} := \text{arg max}_{q \in \mathcal{Q}} \ \text{ELBO}(q)$$

Now, so far we have assumed that $\theta$ is fixed. Is it possible to find both $q$ and $\theta$ jointly? As we discuss in a [previous post on variational inference](https://mbernste.github.io/posts/reparameterization_vi/), it is valid to define the ELBO as a function of _both_ $q$ and $\theta$ and then maximize the ELBO with respect to both of these parameters:

$$\hat{q}, \hat{\theta} := \text{arg max}_{q, \theta} \ \text{ELBO}(q, \theta)$$

The reason for this is that the ELBO is a _lower bound_ on the marginal log-likelihood $p_\theta(x_1, \dots, x_m)$ and thus, optimizing the ELBO with respect to $\theta$ increases the lower bound of the log-likelihood. 


### Variational family used by VAEs

VAEs use a variational family with the following form:

$$\mathcal{Q} := \{N(h^{(1)}_\phi(\boldsymbol{x}), \exp(h^{(2)}\phi(\boldsymbol{x})) \boldsymbol{I}) \mid \phi \in \mathbb{R}^R\}$$

where $h^{(1)}_\phi$ and $hh^{(1)}_\phi$ are two neural networks that map the original object, $\boldsymbol{x}$, to the mean, $\boldsymbol{\mu}$, and the logarithm of the variance, $\log \boldsymbol{\sigma}^2$, of the approximate posterior respectively. Here, $R$ is the number of parameters to these neural networks. Said a second way, we define $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$ as

$$q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) := N(h^{(1)}_\phi(\boldsymbol{x}), \exp(h^{(2)}_\phi(\boldsymbol{x})) \boldsymbol{I})$$

Said a third way, the approximate posterior distribution can be sampled via the following process:

$$\begin{align*}\boldsymbol{\mu} &:=  h^{(1)}_\phi(\boldsymbol{x}) \\ \log \boldsymbol{\sigma}^2 &:= h^{(2)}_\phi(\boldsymbol{x}) \\ \boldsymbol{z} &\sim N(\boldsymbol{\mu}, \boldsymbol{\sigma^2}\boldsymbol{I}) \end{align*}$$

This can be visualized as follows:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_encoder.png" alt="drawing" width="700"/></center>

&nbsp;

Thus, maximizing the ELBO over $\mathcal{Q}$ reduces to maximizing the ELBO over the neural network parameters $\phi$ (in addition to $\theta$ as discussed previously):

$$\begin{align*}\hat{\phi}, \hat{\theta} &= \text{arg max}_{\phi, \theta} \  \text{ELBO}(\phi, \theta) \\ &:= \text{arg max}_{\phi, \theta} \  \sum_{i=1}^m E_{\boldsymbol{z}_i \sim q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i)}\left[ \log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \log q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \end{align*}$$

One detail to point out here is that the approximation of the posterior over each $\boldsymbol{z}\_i$ is defined by a set of parameters $\phi$ that are shared accross all samples $\boldsymbol{z}\_1, \dots, \boldsymbol{z}\_m$. That is, we use a single set of neural network parameters $\phi$ to encode the posterior distribution $q\_\phi(\boldsymbol{z}\_i \mid \boldsymbol{x}\_i)$. Note, we _could_ have gone a different route and defined a _separate_ variational distribution $q\_i$ for each $\boldsymbol{z}\_i$ that is not conditioned on $\boldsymbol{x}\_i$. That is, to define the variational posterior as $q\_{\phi_i}(\boldsymbol{z}\_i)$, where each $\boldsymbol{z}\_i$ has its own set of parameters $\phi\_i$. Here, $q\_{\phi\_i}(\boldsymbol{z}\_i)$ does not condition on $\boldsymbol{x}\_i$. Why don't we do this instead? The answer is that for extremely large datasets it's easier to perform VI when $\phi$ are shared across all data points because it reduces the number of parameters we need to search over in our optimization. This act of defining a common set of parameters shared across all of the independent posteriors is called **amortized variational inference**.

### Maximizing the ELBO

Now that we've set up the optimization problem, we need to solve it. Unfortunately, the expecation present in the ELBO makes this difficult as it requires integrating over all possible values for $\boldsymbol{z}_i$:

$$\begin{align*}\text{ELBO}(\phi, \theta) &= \sum_{i=1}^m E_{\boldsymbol{z} \sim q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}\left[ \log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \log q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \\ &= \sum_{i=1}^m \int_{\boldsymbol{z}_i}   q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \left[ \log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \log q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \ d\boldsymbol{z}_i \end{align*}$$

We address this challenge by using the **reparameterization gradient** method, which we presented in a [previous blog post](https://mbernste.github.io/posts/reparameterization_vi/). We will review this method here; however see my previous post for a more detailed explanation. 

In brief, the reparameterization method maximizes the ELBO via stochastic gradient ascent in which stochastic gradients are formulated by first performing the **reparameterization trick** followed by Monte Carlo sampling.  The reparameterization trick works as follows: we "reparameterize" the distribution $q_\phi(z_i \mid x_i)$ in terms of a surrogate random variable $\epsilon_i \sim \mathcal{D}$ and a determinstic function $g$ in such a way that sampling $z$ from $q_\phi(z_i \mid x_i)$ is performed as follows:

$$\begin{align*}\epsilon_i &\sim \mathcal{D} \\ z_i &:= g_\phi(\boldsymbol{\epsilon}_i, x_i)\end{align*}$$

One way to think about this is that instead of sampling $z$ directly from our variational posterior $q_\phi(z_i \mid x_i)$, we "re-design" the generative process of $z_i$ such that we first sample a surrogate random variable $\epsilon\_i$ and then transform $\epsilon\_i$ into $z_i$ all while ensuring that in the end, the distribution of $z\_i$ still follows $q_\phi(z_i \mid x_i)$. Following the reparameterization trick, we can re-write the ELBO as follows:

$$\text{ELBO}(\phi, \theta) := \sum_{i=1}^m E_{\epsilon_i \sim \mathcal{D}}\left[ \log p_\theta(\boldsymbol{x}_i, g_\phi(\boldsymbol{\epsilon}_i, \boldsymbol{x}_i) - \log q_\phi(g_\phi(\boldsymbol{\epsilon}_i, \boldsymbol{x}_i) \mid \boldsymbol{x}_i) \right]$$

We then approximate the ELBO via Monte Carlo sampling. That is, for each sample, $i$, we first sample random variables from our surrogate distribution $\mathcal{D}$:

$$\boldsymbol{\epsilon}'_{i,1}, \dots, \boldsymbol{\epsilon}'_{i,L} \sim \mathcal{D}$$

Then we can compute a Monte Carlo approximation to the ELBO:

$$\tilde{\text{ELBO}}(\phi, \theta)  \sum_{i=1}^m \sum_{l=1}^L \left[ \log p_\theta(\boldsymbol{x}_i, g_\phi(\boldsymbol{\epsilon}'_{i,l}, \boldsymbol{x}_i) - \log q_\phi(g_\phi(\boldsymbol{\epsilon}'_{i,l}, \boldsymbol{x}_i) \mid \boldsymbol{x}_i) \right]$$

Now the question becomes, what reparamterization can we use? Recall that for the VAEs discussed here, our $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$ is a normal distribution:

$$q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) := N(f_\phi(\boldsymbol{x}), \exp(h_\phi(\boldsymbol{x})) \boldsymbol{I})$$

This naturally can be reparameterized as:

$$\begin{align*}\boldsymbol{\epsilon}_i &\sim N(\boldsymbol{0}, \boldsymbol{I}) \\ z_i &:= f_\phi(\boldsymbol{x}) + \exp(h_\phi(\boldsymbol{x}))\boldsymbol{\epsilon}_i \end{align*}$$

Thus, our function $g$ is simply the function that shifts $\boldsymbol{\epsilon}_i$ by $g_\phi(\boldsymbol{x})$ and scales it by $\exp(h_\phi(\boldsymbol{x}))$. That is, 

$$g(\boldsymbol{\epsilon}_i, \boldsymbol{x}_i) := f_\phi(\boldsymbol{x}) + \exp(h_\phi(\boldsymbol{x}))\boldsymbol{\epsilon}_i$$

Because $\tilde{\text{ELBO}}(\phi, \theta)$ is differentiable with respect to both $\phi$ and $\theta$ (notice that $f_\phi(\boldsymbol{x}_i)$ and $h_\phi(\boldsymbol{x}_i)$ are neural networks which are differentiable), we can form the gradient:

$$\nabla_{\phi, \theta} \tilde{\text{ELBO}}(\phi, \theta) = \sum_{i=1}^m \sum_{l=1}^L \nabla_{\phi, \theta} \left[ \log p_\theta(\boldsymbol{x}_i, g_\phi(\epsilon'_{i,l}, \boldsymbol{x}_i) - \log q_\phi(g_\phi(\epsilon'_{i,l}, \boldsymbol{x}_i) \mid \boldsymbol{x}_i) \right]$$

This gradient can then be used to perform gradient ascent. To compute this gradient, we can apply [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) algorithms in combination with [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)-based optimization, thus enabling us to utilize the full deep learning optimization toolkit! 

### Reducing the variance of the stochastic gradients  

For the VAE model there is a modification that we can make to reduce the variance of the Monte Carlo gradients. We first re-write the original ELBO in a different form:

$$\begin{align*}\text{ELBO}(\phi, \theta) &= \sum_{i=1}^m E_{z_i \sim q} \left[ \log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \log q( \boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \\ &= \sum_{i=1}^m \int q(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \left[\log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \log q(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \ d\boldsymbol{z}_i \\ &= \sum_{i=1}^m \int q(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \left[\log p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}_i) + \log p(\boldsymbol{z}_i) - \log q(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \ d\boldsymbol{z}_i \\ &= \sum_{i=1}^m E_{\boldsymbol{z}_i \sim q} \left[ \log p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}_i) \right] + \sum_{i=1}^m \int q(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \left[\log p(\boldsymbol{z}_i) - \log q(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \ d\boldsymbol{z}_i \\ &= \sum_{i=1}^m E_{\boldsymbol{z}_i \sim q} \left[ \log p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}_i) \right] - \sum_{i=1}^m E_{\boldsymbol{z}_i \sim q}\left[ \int q(\boldsymbol{z}_i \mid \boldsymbol{x}_i)\log \frac{ p(\boldsymbol{z}_i)}{q(\boldsymbol{z}_i \mid \boldsymbol{x}_i)} \right] \\ &= \sum_{i=1}^m E_{\boldsymbol{z}_i \sim q} \left[ \log p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}_i) \right] - KL(q(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \ || \ p(\boldsymbol{z}_i)) \end{align*}$$ 

Recall the VAEs we have considered in this blog post have defined $p(\boldsymbol{z})$ to be the standard normal distribution $N(\boldsymbol{0}, \boldsymbol{I})$. In this particular case, it turns out that the KL-divergence term above can be expressed analytically (See Theorem 1 in the Appendix to this post):

$$KL(q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \mid\mid p(\boldsymbol{z})) = -\frac{1}{2} \sum_{j=1}^J \left(1 + h_\phi(\boldsymbol{x}_i)_j - g_\phi(\boldsymbol{x}_i)_j^2 - h_\phi(\boldsymbol{x}_i)_j \right)$$

Note above the KL-divergence is calculated by summing over each dimension in the latent space. The full ELBO is:

$$\begin{align*} \text{ELBO}(\phi, \theta) &= \sum_{i=1}^m \left[\frac{1}{2} \sum_{j=1}^J \left(1 + h_\phi(\boldsymbol{x}_i)_j - g_\phi(\boldsymbol{x}_i)_j^2 - h_\phi(\boldsymbol{x}_i)_j \right) + E_{\boldsymbol{z}_i \sim q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i)} \left[\log p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}_i) \right]\right] \end{align*}$$

Then, we can apply the reparameterization trick to this formulation of the ELBO and derive the following Monte Carlo approximation:

$$\begin{align*}\text{ELBO}(\phi, \theta) &\approx \sum_{i=1}^m \left[\frac{1}{2} \sum_{j=1}^J \left(1 + h_\phi(\boldsymbol{x}_i)_j - g_\phi(\boldsymbol{x}_i)_j^2 - h_\phi(\boldsymbol{x}_i)_j \right) + \frac{1}{L} \sum_{l=1}^L \left[\log p_\theta(\boldsymbol{x}_i \mid g_\phi(\boldsymbol{x}_{i}) + h_\phi(\boldsymbol{x}_i)\epsilon_{i,l}) \right] \right]\end{align*}$$

We can then apply automatic differentation to this function in order to derive the gradients needed to perform gradient ascent on the ELBO.

Why does this stochastic gradient have reduced variance than the version discussed previously? Intuitively, terms within the ELBO's expectation are being "pulled out" and computed analytically (i.e., the KL-divergence) and thus, there will be less overall variability.

VAEs as autoencoders
---------------------

So far, we have described VAEs in the context of probabilistic modeling. That is, we have described how the VAE is a probabilistic model that describes each high-dimensional datapoint, $\boldsymbol{x}_i$, as being "generated" from a lower dimensional data point $\boldsymbol{z}_i$. This generating procedure utilizes a neural network to map  $\boldsymbol{z}_i$ to the parameters of the distribution $\mathcal{D}$ required to sample $\boldsymbol{x}_i$. Moreover, we can infer the parameters and latent variables to this model via VI. To do so, we solve a sort of inverse problem in which use a neural network to map each $\boldsymbol{x}_i$ into parameters of the variational posterior distribution $q$ required to sample $\boldsymbol{z}_i$. 

Now, what happens if we tie the variational posterior $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$ to the data generating distribution $p_\theta(\boldsymbol{x} \mid \boldsymbol{z})$? That is, given a data point $\boldsymbol{x}$, we first sample $\boldsymbol{z}$ from the variational posterior distribution,

$$\boldsymbol{z} \sim q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$$

then we generate a new data point, $\boldsymbol{x}'$, from $p(\boldsymbol{x} \mid \boldsymbol{z})$:

$$\boldsymbol{x}' \sim p_\theta(\boldsymbol{x} \mid \boldsymbol{z})$$

We can visualize this process schematically below: 

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_as_autoencoder.png" alt="drawing" width="800"/></center>

&nbsp;

Notice the similarity of the above process to the standard autoencoder:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/autoencoder.png" alt="drawing" width="350"/></center>

&nbsp;

Recall that the goal of autoencoders is to learn a function for compressing each $\boldsymbol{x}$_i into a low dimensional "representation" $\boldsymbol{z}_i$ such that we can recover $\boldsymbol{x}$_i from this compressed representation. It does so by minimizing a **reconstruction loss** function such as the mean squared error:

$$\text{loss}_{AE} := \frac{1}{m} \sum_{i=1}^m ||\boldsymbol{x}_i - f_\theta(g_\phi(\boldsymbol{x}_i)) ||_2^2$$

where $g_\phi$ is the encoding neural network and $f_\theta$ is the decoding neural network.

Although the loss function for the VAE is different, it performs the same sort of compression and decompression as a standard autoencoder! One can view the VAE as a "probabilistic" autoencoder. Instead of mapping each $\boldsymbol{x}\_i$ directly to $\boldsymbol{z}\_i$, the VAE maps $\boldsymbol{x}\_i$ to a _distribution_ over $\boldsymbol{z}\_i$ from which $\boldsymbol{z}\_i$ can be _sampled_. This randomly sampled $\boldsymbol{z}\_i$ is then to a distribution over $\boldsymbol{x}\_i$ which can also then be sampled.

Examining the loss function of the VAE
--------------------------------------

Let's take a loser look at the loss function for the VAE with our new view of VAEs as a probabilistic autoencoder. The (exact) loss function is the negative of the ELBO:

$$\begin{align*} \text{loss}_{\text{VAE}}(\phi, \theta) &= -\sum_{i=1}^m E_{\boldsymbol{z}_i \sim q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i)} \left[\log p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}_i) \right] - KL(q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \ || \ p(\boldsymbol{z}_i) \end{align*}$$

Notice there are two terms with opposite signs. The first term, $\log p\_\theta(\boldsymbol{x}\_i \mid \boldsymbol{z}\_i)$, can be seen as a  **reconstruction loss**, because it will drive the model to reconstruct the original $\boldsymbol{x}\_i$ from the its "compressed" representation, $\boldsymbol{z}\_i$. In constract, the KL-divergence term will push the model to generate latent variables, from $q\_\phi(\boldsymbol{z}\_i \mid \boldsymbol{x}\_i)$, that follow the prior distribution $p(\boldsymbol{z}\_i)$, which in our case is a standard normal. Notice that this KL-term has opposite sign to the likelihood term. We can think of this KL-term as a **regularization term** for the reconstruction loss. That is, the model seeks to reconstruct each $\boldsymbol{x}\_i$; however, it also seeks to ensure that the latent $\boldsymbol{z}\_i$'s are distributed according to a standard normal distribution!

Implementation of VAEs versus standard autoencoders
---------------------------------------------------

Though VAE and autoencoder look quite different, their actual implementation is remarkably similar. To see this similarity, let's examine the computation graph of the loss function that we would use to train each model. For the standard autoencoder, the computation graph looks like:

For a VAE it looks like:

There are really only two main differences:
1. The loss function between the two is different (MSE versus the approximated ELBO)
2. We add steps to randomly sample $\boldsymbol{\epsilon}_i$ in order to generate $\boldsymbol{z}_i$ 

As will be shown later in this post, the code in PyTorch used to implement a simple VAE and autoencoder are quite similar.

Example: Training a VAE on MNIST
--------------------------------

I implemented a VAE in [PyTorch](https://pytorch.org) and trained it on the MNIST dataset. These data consist of 28x28 pixel images of hand written digits. My VAE used a latent representation of length 100 (that is, $\boldsymbol{z} \in \mathbb{R}^{100}$.

After enough training, the algorithm was able to reconstruct images that were not included in the training data. Here's an example:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_MNIST_reconstruction.png" alt="drawing" width="600"/></center>

&nbsp;

As you can see, it did pretty well! The reconstructed image looks very much like the original image. This is notable because the input image was not in the training set. Thus, the model learned how to generalize the task of encoding and decoding samples.

Let's see what happens when we attempt to reconstruct images from their latent representation. In order to do so, we first sample $\boldsymbol{z} \sim N(\boldsymbol{0}, \boldsymbol{I})$ and then display $f_\theta(\boldsymbol(z))$:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/example_MNIST_generative_process.png" alt="drawing" width="900"/></center>

&nbsp;

As you can see, some of these digits resemble numbers. Notably, nobody has actually written these digits. They were drawn by the model!

Lastly, let's explore the latent space learned by the model. First, let's take the images of a "9" and "0", and encode them into the latent space by sampling from $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$. Let's let $\boldsymbol{z}_1$ and $\boldsymbol{z}_2 be the latent vectors for "9" and "0" respectively: 

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_encode_9_0.png" alt="drawing" width="800"/></center>

&nbsp;

Then, let's interpolate between $\boldsymbol{z}\_1$ and $\boldsymbol{z}\_2$ and for each interpolated vector $\boldsymbol{z}'$ we'll compute $f_\theta(\boldsymbol{z})$:  

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_interpolate_9_to_0.png" alt="drawing" width="400"/></center>

&nbsp;


Interestingly, we see a smooth transition between the 9 to the 0! The latent representations that exist between 9 and 0 appear as sort of combined forms of these digits. 


Why VAEs over standard autoencoders?
------------------------------------


VAEs in computational biology: the scVI model
---------------------------------------------

